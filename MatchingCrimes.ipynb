{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "generous-peninsula",
   "metadata": {},
   "source": [
    "# Scraping Part - Matching Crimes to Cities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mathematical-egyptian",
   "metadata": {},
   "source": [
    "### The three urls where I get the data from:\n",
    "    - \"https://cnwoman-bot.github.io/evil-man/\"\n",
    "        \n",
    "        This webpage is a online 'bot' page created by Chinese female netizens where the bot receives and lists information about gender-specific crimes in China. Though it is not an official source, most of the documented crimes have urls to their news reports on mainstream media and are well-known on social media, making it a reliable source of information. By the time of scraping, the latest update on the webpage was on 18th Feb, further updates may be added to the webpage but not included in our scrape data.\n",
    "        \n",
    "    - \"https://raw.githubusercontent.com/adyliu/china_area/master/area_code_2020.json\" \n",
    "        \n",
    "        The json file is an outcome of the project here:https://github.com/adyliu/china_area, which contains data of chinese cities. The author used data from the China statistic bureau which should be reliable.\n",
    "\n",
    "    - \"https://raw.githubusercontent.com/WenryXu/ChinaUniversity/master/ChinaUniversityList.json\"\n",
    "        \n",
    "        The json file is an outcome of the project here:https://github.com/WenryXu/ChinaUniversity, which contains data of chinese universities. The author used data from the Education bureaus and WikiPedia, which should be reliable.\n",
    "        \n",
    "### Translations of Chinese words in the code (other than manual coding crime cases):\n",
    "    - '判决日期' means 'sentence date'\n",
    "    - '本小节合计' means 'the sum of this section is'\n",
    "    - '知乎问题' means 'Zhihu Questions' which is a name of a Chinese website\n",
    "    - '台湾' means 'Taiwan'\n",
    "    - '开往' means 'drive to'\n",
    "    - '火车' means 'train'\n",
    "    - '火车站' means 'train station'\n",
    "    - '市辖区' means 'city districts'\n",
    "    - '行政区划' means 'administrative districts'\n",
    "    - '香港' means 'Hong Kong' \n",
    "    - '澳门' means 'Macau'\n",
    "    - '大学' means 'university'\n",
    "    - '学院' means 'college'\n",
    "    - '北大' means 'Peking University'\n",
    "    - '清华' means 'Tsinghua University'\n",
    "    - '市' means 'city', so 'xx市' means 'the city of xx'\n",
    "    - '省' means 'province'\n",
    "    - '首都' means 'capital'\n",
    "    - '地址' means 'address'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pretty-magnitude",
   "metadata": {},
   "source": [
    "## Crime Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "stone-recycling",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import cpca\n",
    "import jieba\n",
    "from china_cities import *\n",
    "import pinyin\n",
    "import csv\n",
    "\n",
    "\n",
    "r = requests.get(\"https://cnwoman-bot.github.io/evil-man/\")\n",
    "soup = BeautifulSoup(r.text,'html.parser')\n",
    "soup.prettify()\n",
    "\n",
    "\n",
    "# getting and modifying the crime list\n",
    "\n",
    "# This function is to get all the crimes listed under a series of <div> tags,\n",
    "# where the crimes are described in the text of <a> tags and linked to urls.\n",
    "def get_news_within(div):\n",
    "    '''\n",
    "    Getting the text within a certain div tag\n",
    "\n",
    "    Input:\n",
    "        div: the certain tag\n",
    "\n",
    "    Output:\n",
    "        lst_text: (list) a list of all text under the div tag\n",
    "    '''\n",
    "    p = div.next_sibling.next_sibling\n",
    "    lst_text = [a.text for a in p.find_all('a')]\n",
    "    return lst_text\n",
    "\n",
    "# This part deals with a small amount of cases that were\n",
    "# listed in a different manner, where rather than the crimes,\n",
    "# the sentences were linked with urls and the crimes were described before,\n",
    "# hence requiring a different pass to retrieve the crimes.\n",
    "# '判决日期' means 'sentence date' which is used to locate crimes.\n",
    "text = [i.parent.text.split('\\n') for i in soup.find_all('a')\n",
    "        if i.text.startswith('判决日期')]\n",
    "\n",
    "other1 = []\n",
    "for t in text:\n",
    "    if len(t) <= 3:\n",
    "        # t is a list of strings, and normally no more than three\n",
    "        # strins were used to describe a case. This is to distinguish\n",
    "        # between t that includes information about more than one case.\n",
    "        other1.append(t[0])\n",
    "    else:\n",
    "        # here we consider the lists describing more than one case.\n",
    "        for s in t:\n",
    "            if '判决日期' not in s:\n",
    "                other1.append(s)\n",
    "\n",
    "other1 = list(set(other1))  #avoiding duplicates\n",
    "\n",
    "# These are some manually coded entries that are embedded\n",
    "# in general narration or discussion on the webpage,\n",
    "# which do not follow particular patterns.\n",
    "other2 = ['新疆喀什大学文学院援疆博士王明科',\n",
    "          '云南云南大学文学院副教授蔡英杰性骚扰留学生被公开处分，现任福建师范大学教授',\n",
    "          '甘肃兰州大学大气科学学院副院长张文煜被曝性侵',\n",
    "          '上海戏剧学院教务处处长厉震林性骚扰，已无链接',\n",
    "          '央美设计学院院长宋协伟被指控性骚扰、猥亵女学生',\n",
    "          '原北京中央民族大学文学院与新闻传播学院刘立刚性骚扰',\n",
    "          '湖北美院偷拍女生裙底把视频发到了p站，无语，链接不放了',\n",
    "          '北京电影学院阿廖莎性侵案',\n",
    "          '北京电影学院侯亮平事件',\n",
    "          '亳州6·15杀人案,邻居眼中的老实人,为何连杀4人',\n",
    "          '四川一名大学本科男子刘某在成都开往深圳的火车上猥亵9岁女童',\n",
    "          '浙大学生努某某强奸被判一年半，学校不与开除，留校察看',\n",
    "          '猥亵女童的网红“豆浆王子”蒙顺宁，出狱后获得公益组织爱心帮扶',\n",
    "          '西南某大学法学院22岁男生偷拍女生裙底',\n",
    "          '陕西安康石泉30岁的男子索爱一女子不成跳江自杀，女子被判决赔偿各项损失七万元']\n",
    "\n",
    "# '本小节合计' means 'the sum of this section', which can locate the <div> tags\n",
    "# with crimes listed under them.\n",
    "div = [d for d in soup.find_all('div',class_ = \"language-plaintext highlighter-rouge\")\n",
    "       if '本小节合计' in d.text]\n",
    "crime = []\n",
    "for d in div:\n",
    "    crime += get_news_within(d)\n",
    "crime += other1\n",
    "crime += other2\n",
    "\n",
    "# removing crimes where the <a> text is the name of the webpage ('知乎问题')\n",
    "# where the crime was reported; in these cases the crimes themselves\n",
    "# were part of the narration and hence manually added\n",
    "crime = [c for c in crime if c != '知乎问题']\n",
    "\n",
    "# removing crimes happened in Taiwan, Hong Kong, Macau \n",
    "# as our other datasets do not include them\n",
    "crime = [l for l in crime if '台湾' not in l and '香港' not in l and '澳门' not in l]\n",
    "\n",
    "# manually dropping crimes happened overseas\n",
    "# and cases that are not about particular incidents\n",
    "manual_drop = ['澳洲华人男子罗浩灵疑残忍杀妻，藏尸公寓冰柜！带俩娃逃回国，刚刚被捕',\n",
    "                '澳洲华人男子胡伟杀妻后自杀，8岁儿子在旁目睹全过程',\n",
    "                '美国纽约州威彻斯特郡46岁的华裔男子刘传凯杀死妻子和俩孩子后自杀身亡',\n",
    "                '新加坡一中国籍男子Cui Huan谋杀妻子，罪成将面对死刑或终身监禁兼鞭刑',\n",
    "                '泰国华裔男子喂刚出生7天的女儿洁厕剂，杀害并埋尸坟地',\n",
    "                '留日女学生江歌被闺蜜刘鑫前男友陈世峰捅刀杀死',\n",
    "                '澳大利亚国立大学中国留学生Junqi Huang浴室偷拍被捕，遭校方开除。曾任超级脑学会的前任主席',\n",
    "                '美国伊利诺伊大学厄巴纳-香槟分校华人教授徐钢性侵女学生',\n",
    "                '我是包丽的朋友，真相远比你知道的更可怕',\n",
    "                '日本东京街头一中国籍男子强摸女性被捕',\n",
    "                '女友爆料罗志祥多次出轨，参与多人群p',\n",
    "                '女医护人员剃光头“是否自愿”遭质疑，网友：唯一男医生是平头',\n",
    "                '只要我不报道女性，不宣传女性，所有的功劳就还是男性的',\n",
    "                '“洋媳妇”与“洋女婿”双重标准纪实',\n",
    "                '各项扶贫政策落实后，某贫困户要求给他分配个女人',\n",
    "                '“致良知四合院”会场宣传女性应该完全服从男性',\n",
    "                '女德班成果：“中国女孩只属于中国男孩”',\n",
    "                '泰国乌汶府Pha Taem国家公园一江苏男子俞某杀妻谋财',\n",
    "                '泰国春武里府的海滩，中国台湾男子卢子扬杀妻抛尸',\n",
    "                '泰国一家酒店的泳池中天津男子张轶凡杀妻骗保',\n",
    "                '韩国一中国籍游客男子陈某在天主教堂杀害一名女路人']\n",
    "\n",
    "crime = [c for c in crime if c not in manual_drop]\n",
    "\n",
    "# dropping cases happened on cross-city transportations as the city would be difficult to identify\n",
    "# '开往' means 'drive to','火车' means train and '火车站' means train station.\n",
    "# We still keep the cases happened in stations as those cities are identifiable.\n",
    "transport = [c for c in crime if '开往' in c or ('火车' in c and '火车站' not in c)]\n",
    "crime = [c for c in crime if c not in transport]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "primary-diploma",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1766"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(crime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considered-relationship",
   "metadata": {},
   "source": [
    "## City Dictionary Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "banned-migration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the city dictionary from an existent json file\n",
    "\n",
    "#The keys are cities and the values are corresponding made up of\n",
    "#three components:\n",
    "#the city's subordinate units in both abbreviated version and full version,\n",
    "#the city in both abbreviated version and full version,\n",
    "#its province (if any) in both abbreviated version and full version.\n",
    "\n",
    "url = (\"https://raw.githubusercontent.com/adyliu\"\n",
    "       \"/china_area/master/area_code_2020.json\")\n",
    "df = pd.read_json(url)\n",
    "dct_city={}\n",
    "\n",
    "# Location units have five different levels in China, from province \n",
    "# to villages; we only consider cities at level 1 and 2 as they align \n",
    "# with the general perception of cities better.\n",
    "for row in df.iterrows():\n",
    "    prov = row[1]['name']\n",
    "    for dct in row[1]['children']:\n",
    "        if dct['name'] == '市辖区' and dct['level'] == 2:\n",
    "            # if cities are level 1, they don't belong to\n",
    "            # any province so their 'name' in the 'children'\n",
    "            # would be '市辖区' ('city districts'); here the\n",
    "            # province would be the city\n",
    "            key = prov\n",
    "        elif dct['level'] == 2:\n",
    "            # considering only level 2 cities now\n",
    "            key = dct['name']\n",
    "        if key.endswith('行政区划') == False and len(key) > 1:\n",
    "            # '行政区划' also means districts rather than cities\n",
    "            # so we drop these terms.\n",
    "            # Because official names are used here,\n",
    "            # each city is named with the word 'city'('市'),\n",
    "            # hence making it at least 2 characters long\n",
    "\n",
    "            # finding the subordinate units, e.g. counties\n",
    "            rel_city = [d['name'] for d in dct['children']]\n",
    "\n",
    "            # adding prov as a related unit\n",
    "            rel_city.append(prov)\n",
    "\n",
    "            # adding usual abbreviations of city names as they\n",
    "            # are used often in news\n",
    "            sim_city = [c[:-1] for c in rel_city if len(c) >= 3]\n",
    "            rel_city += sim_city\n",
    "\n",
    "            # adding the city's abbreviation\n",
    "            rel_city.append(key[:-1])\n",
    "\n",
    "            dct_city[key] = set(rel_city)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proper-terry",
   "metadata": {},
   "source": [
    "## University Dictionary Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "governmental-corrections",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a university dictionary as some cases were located in universities\n",
    "\n",
    "# The keys are cities and the values are universities in that city\n",
    "\n",
    "url = (\"https://raw.githubusercontent.com/WenryXu/\"\n",
    "        \"ChinaUniversity/master/ChinaUniversityList.json\")\n",
    "df_uni = pd.read_json(url)\n",
    "dct_uni = {}\n",
    "\n",
    "for l in df_uni.schools:\n",
    "    for d in l:\n",
    "        c = d['city']\n",
    "        n = d['name']\n",
    "        if c in dct_uni.keys():\n",
    "            dct_uni[c] += [n]\n",
    "        else:\n",
    "            dct_uni[c] = [n]\n",
    "            \n",
    "# remove universities in Taiwan, Hong Kong\n",
    "# and Macau, because we do not consider these \n",
    "# cases and there are Chinese universities\n",
    "# sharing the sames\n",
    "del dct_uni['台湾省']\n",
    "del dct_uni['香港特别行政区']\n",
    "del dct_uni['澳门特别行政区']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quick-feature",
   "metadata": {},
   "source": [
    "## Matching Crimes to Cities\n",
    "We first match crimes based on universities, because universities can be more precisely located, and there exist cases where universities are named after a city/province but located elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "amino-spring",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/80/_bz7_r210nv91yqrsm9c_pt00000gn/T/jieba.cache\n",
      "Loading model cost 1.269 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "# matching crimes\n",
    "\n",
    "left_crime = crime[:]\n",
    "lst_ma = []\n",
    "dct_m = {}\n",
    "\n",
    "def match_uni(string,u):\n",
    "    '''\n",
    "    Matching a crime case string to a city\n",
    "    \n",
    "    Input:\n",
    "        string: (string) the crime string\n",
    "        u: (string) the keyword ('大学university'/'学院college') \n",
    "            to help locate university names\n",
    "    '''\n",
    "    city = None\n",
    "    ind = [i for i in range(len(string)) if string[i] == u[0]][0]\n",
    "    key = string[0:ind + 2]\n",
    "    if key in lst_val:\n",
    "        # if university name is the first term in the string\n",
    "        # This is a valid assumption as news are usually reported \n",
    "        # with location upfront.\n",
    "        city = [k for k in dct_uni.keys() if key in dct_uni[k]][0]\n",
    "        if city in dct_m.keys():\n",
    "            dct_m[city] += 1\n",
    "        else:\n",
    "            dct_m[city] = 1\n",
    "        if string in left_crime:\n",
    "            left_crime.remove(string)\n",
    "            lst_ma.append((string,city))\n",
    "\n",
    "    else:\n",
    "        # if university is not the first term\n",
    "        set_w = jieba.cut(string)\n",
    "        # the jieba library can cut strings into terms that\n",
    "        # make sense and usually full names of universities \n",
    "        # will be one term.\n",
    "        set_uni = {w for w in set_w if u in w}\n",
    "        if len(set_uni) > 0:\n",
    "            # choosing the longest term because even if other\n",
    "            # terms have the keyword within, the full name of \n",
    "            # university would be the longest one\n",
    "            uni = [w for w in set_uni if len(w) == \n",
    "                    max({len(t) for t in set_uni})][0]\n",
    "            if uni in lst_val:\n",
    "                city = [k for k in dct_uni.keys() if uni in dct_uni[k]][0]\n",
    "                if city in dct_m.keys():\n",
    "                    dct_m[city] += 1\n",
    "                else:\n",
    "                    dct_m[city] = 1\n",
    "                left_crime.remove(string)\n",
    "                lst_ma.append((string,city))\n",
    "\n",
    "\n",
    "lst_val = []\n",
    "# creating a list of all universities to see if a university is included \n",
    "# in the dictionary values\n",
    "for v in dct_uni.values():\n",
    "    lst_val += v\n",
    "for l in crime:\n",
    "    if '大学' in l:\n",
    "        # if the term 'university' is included, \n",
    "        # the keyword would be 'university'\n",
    "        match_uni(l,'大学')\n",
    "    elif '学院' in l:\n",
    "        # if the term 'college' is included, \n",
    "        # the keyword would be 'college'\n",
    "        match_uni(l,'学院')\n",
    "    if ('北大' in l or '清华' in l) and l in left_crime:\n",
    "        # '北大' is an abbreviation for PKU and '清华' for THU in Beijing.\n",
    "        # The universities are too well-known that most of the times\n",
    "        # people do not call them by their full names but these abbreviations.\n",
    "        if '北京市' not in dct_m.keys():\n",
    "            dct_m['北京市'] = 1\n",
    "        else:\n",
    "            dct_m['北京市'] += 1\n",
    "        left_crime.remove(l)\n",
    "        lst_ma.append((l,'北京市'))\n",
    "\n",
    "# manually adding cases where university names are abbreviated,\n",
    "# which cannot be matched as these abbreviations are not generated on\n",
    "# a unanimous standard, or where some professors' names are given \n",
    "# but the universities need to be inferred\n",
    "uni_c = '央美国画系教授丘挺多次与多位女学生发生关系，其妻欲跳楼轻生'\n",
    "left_crime.remove(uni_c)\n",
    "dct_m['北京市'] += 1\n",
    "lst_ma.append((uni_c,'北京市'))\n",
    "\n",
    "uni_c = '央美国画系教授姚舜熙被学生联合实名举报x骚扰、以权谋私、诽谤等'\n",
    "left_crime.remove(uni_c)\n",
    "dct_m['北京市'] += 1\n",
    "lst_ma.append((uni_c,'北京市'))\n",
    "\n",
    "uni_c = '北航教授陈小武被举报性骚扰 疑似曾致手下学生怀孕'\n",
    "left_crime.remove(uni_c)\n",
    "dct_m['北京市'] += 1\n",
    "lst_ma.append((uni_c,'北京市'))\n",
    "\n",
    "uni_c = '女学生遭HYD博导凌昌全教授十五年骚扰、打击报复、逼迫就范'\n",
    "left_crime.remove(uni_c)\n",
    "dct_m['上海市'] += 1\n",
    "lst_ma.append((uni_c,'上海市'))\n",
    "\n",
    "uni_c = '哈工大男生在网上用污言秽语辱骂师大女生'\n",
    "left_crime.remove(uni_c)\n",
    "dct_m['哈尔滨市'] = 1\n",
    "lst_ma.append((uni_c,'哈尔滨市'))\n",
    "\n",
    "uni_c = '厦大海洋与海岸带发展研究院在读硕士彭炜敏\\\n",
    "婚内出轨，并假冒母亲身份欺骗女生'\n",
    "left_crime.remove(uni_c)\n",
    "dct_m['厦门市'] += 1\n",
    "lst_ma.append((uni_c,'厦门市'))\n",
    "\n",
    "uni_c = '浙大央美性搔扰惯犯崔青洲'\n",
    "left_crime.remove(uni_c)\n",
    "dct_m['杭州市'] = 1\n",
    "lst_ma.append((uni_c,'杭州市'))\n",
    "\n",
    "uni_c = '央美设计学院院长宋协伟被指控性骚扰、猥亵女学生'\n",
    "left_crime.remove(uni_c)\n",
    "dct_m['北京市'] += 1\n",
    "lst_ma.append((uni_c,'北京市'))\n",
    "\n",
    "uni_c = '浙大学生努某某强奸被判一年半，学校不与开除，留校察看'\n",
    "left_crime.remove(uni_c)\n",
    "dct_m['杭州市'] += 1\n",
    "lst_ma.append((uni_c,'杭州市'))\n",
    "\n",
    "uni_c = '北工大外国语学院男生吴江掐死女友续：凶手被判死缓'\n",
    "left_crime.remove(uni_c)\n",
    "dct_m['北京市'] += 1\n",
    "lst_ma.append((uni_c,'北京市'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "imported-vinyl",
   "metadata": {},
   "outputs": [],
   "source": [
    "# matching other cases by searching for locations\n",
    "def cpca_match(s):\n",
    "    '''\n",
    "    Retriving the city information from a crime string\n",
    "    \n",
    "    Input:\n",
    "        s: (string) the crime string to be analysed\n",
    "        \n",
    "    Output:\n",
    "        city: (string) the city retrieved\n",
    "    '''\n",
    "    df = cpca.transform([s])\n",
    "    row = df.iloc[0,]\n",
    "    city = row['市']\n",
    "    prov = row['省']\n",
    "    if prov != None and prov.endswith('市'): \n",
    "        # this deals with the special cities like Beijing \n",
    "        # that belongs to no province; so the province name\n",
    "        # ends with '市' meaning 'city'\n",
    "        city = prov\n",
    "        return city\n",
    "    else:\n",
    "        if city != None:\n",
    "            if city.endswith('行政区划') == False:\n",
    "                # '行政区划' means 'districts' so if a term\n",
    "                # ends with it, it will not be a city as we define\n",
    "                return city\n",
    "        else:\n",
    "            if '首都' in s:\n",
    "                # '首都' means capital but does not exist in the dictionary keys\n",
    "                city = '北京市'\n",
    "            else:\n",
    "                lst_w = list(jieba.cut(s))\n",
    "                del_ind = len(lst_w[0])\n",
    "                if len(s) > del_ind:\n",
    "                    city = cpca_match(s[del_ind:])\n",
    "                    # this recursion is used here because normally each crime \n",
    "                    # is described with its location upfront, but sometimes \n",
    "                    # the locations can be wrong, leading to the city\n",
    "                    # equalling None,or the locations were stated \n",
    "                    # after some description,so we divide the string \n",
    "                    # into keywords by the jieba library and remove the \n",
    "                    # keywords once at a time until we can locate the city\n",
    "                    # or retrieve no information\n",
    "            return city\n",
    "\n",
    "\n",
    "def county_match(s):\n",
    "    '''\n",
    "    Retriving the city information from a crime string \n",
    "    based on county information (if any)\n",
    "    \n",
    "    Input:\n",
    "        s: (string) the crime string to be analysed\n",
    "        \n",
    "    Output:\n",
    "        city: (string) the city retrieved\n",
    "    '''\n",
    "    row = cpca.transform([s]).iloc[0,]\n",
    "    prov = row['省']\n",
    "    if prov != None and row['市'] == None and row['区'] == None:\n",
    "        # This is the case where cities cannot be identified\n",
    "        # but provinces can, so we focus on the remaining string \n",
    "        # without the province term to avoid provinces interfering\n",
    "        s = row['地址']\n",
    "    else:\n",
    "        # In some cases locations are wrongly matched,\n",
    "        # leading to the '市'/city column or '区'/district\n",
    "        # column not being None, so we disregard all these \n",
    "        # circumstances and analyse the original string.\n",
    "        prov = None\n",
    "    set_w = set(jieba.cut(s))\n",
    "\n",
    "    if prov != None:\n",
    "        # narrowing the possible cities based on provinces\n",
    "        potential = [c for c in dct_city.keys() if prov in dct_city[c]]\n",
    "    else:\n",
    "        potential = dct_city.keys()\n",
    "\n",
    "    # Because most county names are unique, especially\n",
    "    # after pinning down provinces, the cities those values share the \n",
    "    # most common terms with the string term set should be the most\n",
    "    # likely cities.\n",
    "    max_sim = 0\n",
    "    cities = []\n",
    "    city = None\n",
    "    for k in potential:\n",
    "        sim = len(dct_city[k] & set_w)\n",
    "        if sim > max_sim:\n",
    "            max_sim = sim\n",
    "            cities = [k]\n",
    "        elif sim != 0 and sim == max_sim:\n",
    "            cities.append(k)\n",
    "    if len(set(cities)) == 1:\n",
    "        # if there is only one likely city, it will be the city\n",
    "        city = cities[0]\n",
    "    else:\n",
    "        # When more than 1 cities or no cities are included,\n",
    "        # sometimes it is because the abbreviation of the county names\n",
    "        # are differently constructed to most other counties. Because\n",
    "        # the dictionary values are essentially sets of strings,\n",
    "        # abbreviation may not be found when searching through the sets, \n",
    "        # but can be found when searching through strings in the sets,\n",
    "        # which is why this step is conducted separately to the last.\n",
    "        lst_city = [] \n",
    "        key = list(jieba.cut(s))[0]\n",
    "        county_sim = 0\n",
    "        for c in potential:\n",
    "            v = dct_city[c]\n",
    "            ct_sim = len([ct for ct in v if key in ct])\n",
    "            if ct_sim > county_sim:\n",
    "                county_sim = ct_sim\n",
    "                lst_city = [c]\n",
    "            elif ct_sim != 0 and ct_sim == county_sim:\n",
    "                lst_city.append(c)\n",
    "        if len(lst_city) == 1:\n",
    "            city = lst_city[0]\n",
    "    return city\n",
    "\n",
    "leftover = left_crime[:]\n",
    "for c in leftover:\n",
    "    city = cpca_match(c)\n",
    "    if city == None:\n",
    "        city = county_match(c)\n",
    "    if city != None:\n",
    "        if city in dct_m.keys():\n",
    "            dct_m[city] += 1\n",
    "        else:\n",
    "            dct_m[city] = 1\n",
    "        left_crime.remove(c)\n",
    "        lst_ma.append((c,city))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "endangered-grain",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually editing some wrongly matched crimes mainly\n",
    "# because some criminals' names are also city names,\n",
    "# or there are multiple locations within the sentence \n",
    "# and the first is not the crime location, or cannot be matched \n",
    "# with the above codes\n",
    "\n",
    "ind = [i for i in range(len(lst_ma)) if lst_ma[i][0] == \n",
    "        '辽宁凌源籍男子占某在江苏南通市市区钟楼广场地下通道处偷袭猥亵过路女子'][0]\n",
    "lst_ma[ind] = ('辽宁凌源籍男子占某在江苏南通市市区钟楼广场地下通道处\\\n",
    "偷袭猥亵过路女子','南通市')\n",
    "dct_m['南通市'] += 1\n",
    "dct_m['朝阳市'] -= 1\n",
    "\n",
    "ind = [i for i in range(len(lst_ma)) if lst_ma[i][0] == \n",
    "        '甘肃青海大通县多林镇吴仕庄村男子蔡某与妻子不和，杀妻子全家后自杀'][0]\n",
    "lst_ma[ind] = ('甘肃青海大通县多林镇吴仕庄村男子蔡某与妻子\\\n",
    "不和，杀妻子全家后自杀','西宁市')\n",
    "dct_m['西宁市'] = 1\n",
    "dct_m['重庆市'] -= 1\n",
    "\n",
    "ind = [i for i in range(len(lst_ma)) if lst_ma[i][0] == \n",
    "        '辽宁建昌县农贸市场宏大东北大药房一男子王某军持匕首当场捅死前妻'][0]\n",
    "lst_ma[ind] = ('辽宁建昌县农贸市场宏大东北大药房一男子王某军\\\n",
    "持匕首当场捅死前妻','葫芦岛市')\n",
    "dct_m['葫芦岛市'] += 1\n",
    "dct_m['北京市'] -= 1\n",
    "\n",
    "ind = [i for i in range(len(lst_ma)) if lst_ma[i][0] == \n",
    "        '原北大教师沈阳被指“性侵女生致其自杀”'][0]\n",
    "lst_ma[ind] = ('原北大教师沈阳被指“性侵女生致其自杀','南京市')\n",
    "dct_m['南京市'] += 1\n",
    "dct_m['北京市'] -= 1\n",
    "\n",
    "ind = [i for i in range(len(lst_ma)) if lst_ma[i][0] == \n",
    "        '贵州紫云县一父亲强奸20岁亲生女儿长达6年 一审被判12年'][0]\n",
    "lst_ma[ind] = ('贵州紫云县一父亲强奸20岁亲生女儿长达6年 一审被判12年','安顺市')\n",
    "dct_m['安顺市'] = 1\n",
    "dct_m['临沧市'] -= 1\n",
    "\n",
    "ind = [i for i in range(len(lst_ma)) if lst_ma[i][0] == \n",
    "        '山东陵县宋家镇东屯村农民郭永庆掐死女友'][0]\n",
    "lst_ma[ind] = ('山东陵县宋家镇东屯村农民郭永庆掐死女友','德州市')\n",
    "dct_m['德州市'] += 1\n",
    "dct_m['重庆市'] -= 1\n",
    "\n",
    "ind = [i for i in range(len(lst_ma)) if lst_ma[i][0] == \n",
    "        '云南巍山县大仓镇男子段金泉杀害6名年轻女性'][0]\n",
    "lst_ma[ind] = ('云南巍山县大仓镇男子段金泉杀害6名\\\n",
    "年轻女性','大理白族自治州')\n",
    "dct_m['大理白族自治州'] += 1\n",
    "dct_m['重庆市'] -= 1\n",
    "\n",
    "ind = [i for i in range(len(lst_ma)) if lst_ma[i][0] == \n",
    "        '福建厦门一女患者在复旦大学附属医院厦门分院更衣室换衣服被男子观看\\\n",
    "全程，医院：不能说是偷窥！'][0]\n",
    "lst_ma[ind] = ('福建厦门一女患者在复旦大学附属医院厦门分院更衣室换衣服被\\\n",
    "男子观看全程，医院：不能说是偷窥！','厦门市')\n",
    "dct_m['厦门市'] += 1\n",
    "dct_m['上海市'] -= 1\n",
    "\n",
    "ind = [i for i in range(len(lst_ma)) if lst_ma[i][0] == \n",
    "        '福建龙岩武平男子钟某鸣将女子诱骗至其厦门\\\n",
    "暂住处，用其手机申请网络贷款并将其杀害'][0]\n",
    "lst_ma[ind] = ('福建龙岩武平男子钟某鸣将女子诱骗至其厦门\\\n",
    "暂住处，用其手机申请网络贷款并将其杀害','厦门市')\n",
    "dct_m['厦门市'] += 1\n",
    "dct_m['龙岩市'] -= 1\n",
    "\n",
    "l = '江西玉山一湖南籍男子张某猥亵女学生和女老师'\n",
    "left_crime.remove(l)\n",
    "dct_m['上饶市'] += 1\n",
    "lst_ma.append((l,'上饶市'))\n",
    "\n",
    "l = '西南某大学法学院22岁男生偷拍女生裙底'\n",
    "left_crime.append(l)\n",
    "dct_m['黔西南布依族苗族自治州'] -= 1\n",
    "lst_ma.remove((l,'黔西南布依族苗族自治州'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "passive-europe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1610"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lst_ma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "existing-marsh",
   "metadata": {},
   "source": [
    "## Matching Results to English City Names and Write into CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "specific-admission",
   "metadata": {},
   "outputs": [],
   "source": [
    "# matching to English names and writing into csv\n",
    "\n",
    "# constructing a dictionary matching city Chinese names to English ones\n",
    "dct_lang = {}\n",
    "\n",
    "for c in cities.get_cities():\n",
    "    if c.name_cn in dct_m.keys():\n",
    "        dct_lang[c.name_cn] = (c.name_en,c.province)\n",
    "        # Province was added because some cities of different provinces\n",
    "        # share the same English names.\n",
    "\n",
    "# Because different collection of cities, some cities are missing \n",
    "# in the library, so we add them using normal translation norms.\n",
    "lst_special = [c for c in dct_m.keys() if c not in dct_lang.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "opened-divorce",
   "metadata": {},
   "outputs": [],
   "source": [
    "special = lst_special[:]\n",
    "for ct in special:\n",
    "    key = list(jieba.cut(ct,cut_all=True))[0]\n",
    "    # This is to disentangle some measure units like 'city'\n",
    "    # or 'self-governing state' which can be coded differently\n",
    "    # in different library/datasets; but as the measure units are\n",
    "    # always behind the names, selecting the first element\n",
    "    # can usually derive the name.\n",
    "    eng = [(c.name_en,c.province) for c in\n",
    "            cities.get_cities() if key in c.name_cn]\n",
    "    if len(eng) == 1:\n",
    "        dct_lang[ct] = eng[0]\n",
    "        lst_special.remove(ct)\n",
    "    else:\n",
    "        # If the name is not in the dataset, we translate ourselves.\n",
    "        eng_name = pinyin.get(key, format=\"strip\",\n",
    "                     delimiter=\"\").capitalize()\n",
    "        # getting the pinyin of the name\n",
    "        prov = [c for c in dct_city[ct] if c.endswith('省')][0]\n",
    "        # finding its province\n",
    "        prov_eng = pinyin.get(prov[:-1], format=\"strip\",\n",
    "                    delimiter=\"\").capitalize()\n",
    "        # getting the pinyin for the province\n",
    "        dct_lang[ct] = (eng_name,prov_eng)\n",
    "        lst_special.remove(ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "split-qatar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing matching correctness\n",
    "sum(dct_m.values()) == len(lst_ma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "informed-samba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing matching correctness\n",
    "l = []\n",
    "for i in dct_m.keys():\n",
    "    if len([l for l in lst_ma if l[-1] == i]) != dct_m[i]:\n",
    "        l.append(i)\n",
    "len(l) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "governing-somalia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing cities with 0 crimes\n",
    "l_rmkey = []\n",
    "for k in dct_m.keys():\n",
    "    if dct_m[k] == 0:\n",
    "        l_rmkey.append(k)\n",
    "        \n",
    "for k in l_rmkey:\n",
    "    del dct_m[k]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satellite-criticism",
   "metadata": {},
   "source": [
    "We drop the left crimes because they cannot be further matched - we tried requesting the urls but as different webpages use different design, the retrieved cities are quite inaccurate. Since these left crimes take up only about 9% of the total crimes, we believe dropping them would not largely affect our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "apparent-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing into CSV files\n",
    "lst_result = []\n",
    "for k,v in dct_m.items():\n",
    "    tup = (dct_m[k],k,dct_lang[k])\n",
    "    lst_result.append(tup)\n",
    "lst_result.sort()\n",
    "\n",
    "with open(\"data.csv\",\"w\") as csvfile: \n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['number of cases','location_CN','location_ENG'])\n",
    "    for l in lst_result:\n",
    "        writer.writerow(l)\n",
    "\n",
    "with open(\"matches.csv\",\"w\") as csvfile: \n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['case','location'])\n",
    "    for l in lst_ma:\n",
    "        writer.writerow(l)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
